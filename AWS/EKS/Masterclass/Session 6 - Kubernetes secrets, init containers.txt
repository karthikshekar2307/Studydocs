***** Kubernetes - Secrets
Step-01: Introduction
Kubernetes Secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys.
Storing confidential information in a Secret is safer and more flexible than putting it directly in a Pod definition or in a container image.

Step-02: Create Secret for MySQL DB Password
# Mac
echo -n 'dbpassword11' | base64

# URL: https://www.base64encode.org
Create Kubernetes Secrets manifest
apiVersion: v1
kind: Secret
metadata:
  name: mysql-db-password
#type: Opaque means that from kubernetes's point of view the contents of this Secret is unstructured.
#It can contain arbitrary key-value pairs. 
type: Opaque
data:
  # Output of echo -n 'dbpassword11' | base64
  db-password: ZGJwYXNzd29yZDEx


****** Kubernetes - Init Containers
Step-01: Introduction
Init Containers run before App containers
Init containers can contain utilities or setup scripts not present in an app image.
We can have and run multiple Init Containers before App Container.
Init containers are exactly like regular containers, except:
Init containers always run to completion.
Each init container must complete successfully before the next one starts.
If a Pod's init container fails, Kubernetes repeatedly restarts the Pod until the init container succeeds.
However, if the Pod has a restartPolicy of Never, Kubernetes does not restart the Pod.

Step-02: Implement Init Containers
Update initContainers section under Pod Template Spec which is spec.template.spec in a Deployment
  template:
    metadata:
      labels:
        app: usermgmt-restapp
    spec:
      initContainers:
        - name: init-db
          image: busybox:1.31
          command: ['sh', '-c', 'echo -e "Checking for the availability of MySQL Server deployment"; while ! nc -z mysql 3306; do sleep 1; printf "-"; done; echo -e "  >> MySQL DB Server has started";']

Step-03: Create & Test
# Create All Objects
kubectl apply -f kube-manifests/

# List Pods
kubectl get pods

# Watch List Pods screen
kubectl get pods -w

# Describe Pod & Discuss about init container
kubectl describe pod <usermgmt-microservice-xxxxxx>

# Access Application Health Status Page
http://<WorkerNode-Public-IP>:31231/usermgmt/health-status

Step-04: Clean-Up
Delete all k8s objects created as part of this section
# Delete All
kubectl delete -f kube-manifests/

# List Pods
kubectl get pods

# Verify sc, pvc, pv
kubectl get sc,pvc,pv
References:
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/


***** Kubernetes - Liveness & Readiness Probes

3 Different types of Probes

(a) Liveness probes: 
- Kubelet uses liveness probes to know when to restart a container
- Liveness probes could catch a deadlock, where an application is running, but unable to make progress and restarting container helps in such state

(b) Readiness probes
- Kubelet uses readiness probe to kow when a container is ready to accept traffic
- When a Pod is not ready, it is removed from service load balancers based on this readiness probe signal.

(c) Startup probe
- Kubelet uses startup probes to know when a container application has started
- Firstly this probe disables liveness and readiness checks unitl it succeeds enuring those pods dont interfere with app startup
- This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.

Options to define probes
# nc -z localhost 8095
httpget path:/health-status
tcpsocket Port:8095

Step-01: Introduction
Refer Probes slide for additional details

Step-02: Create Liveness Probe with Command
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - nc -z localhost 8095
            initialDelaySeconds: 60
            periodSeconds: 10

Step-03: Create Readiness Probe with HTTP GET
          readinessProbe:
            httpGet:
              path: /usermgmt/health-status
              port: 8095
            initialDelaySeconds: 60
            periodSeconds: 10     


Step-04: Create k8s objects & Test
# Create All Objects
kubectl apply -f kube-manifests/

# List Pods
kubectl get pods

# Watch List Pods screen
kubectl get pods -w

# Describe Pod & Discuss about init container
kubectl describe pod <usermgmt-microservice-xxxxxx>

# Access Application Health Status Page
http://<WorkerNode-Public-IP>:31231/usermgmt/health-status
Observation: User Management Microservice pod witll not be in READY state to accept traffic until it completes the initialDelaySeconds=60seconds.

Step-05: Clean-Up
Delete all k8s objects created as part of this section
# Delete All
kubectl delete -f kube-manifests/

# List Pods
kubectl get pods

# Verify sc, pvc, pv
kubectl get sc,pvc,pv
References:
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/


***** Kubernetes - Requests and Limits
Step-01: Introduction
We can specify how much each container a pod needs the resources like CPU & Memory.
When we provide this information in our pod, the scheduler uses this information to decide which node to place the Pod on.
When you specify a resource limit for a Container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.
The kubelet also reserves at least the request amount of that system resource specifically for that container to use.

Step-02: Add Requests & Limits
          resources:
            requests:
              memory: "128Mi" # 128 MebiByte is equal to 135 Megabyte (MB)
              cpu: "500m" # `m` means milliCPU
            limits:
              memory: "500Mi"
              cpu: "1000m"  # 1000m is equal to 1 VCPU core                                          


Step-03: Create k8s objects & Test
# Create All Objects
kubectl apply -f kube-manifests/

# List Pods
kubectl get pods

# Watch List Pods screen
kubectl get pods -w

# Describe Pod & Discuss about init container
kubectl describe pod <usermgmt-microservice-xxxxxx>

# Access Application Health Status Page
http://<WorkerNode-Public-IP>:31231/usermgmt/health-status

# List Nodes & Describe Node
kubectl get nodes
kubectl describe node <Node-Name>

Step-04: Clean-Up
Delete all k8s objects created as part of this section
# Delete All
kubectl delete -f kube-manifests/

# List Pods
kubectl get pods

# Verify sc, pvc, pv
kubectl get sc,pvc,pv
References:
https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/


***** Kubernetes Namespaces - Imperative using kubectl
- Namespaces ae also called virtual clusters in our physical K8s cluster
- We use this in environments where we have many users spread across multiple teams or projects
- Cluster with tens of users ideally dont need to use namespaces
- Benefits:
creates isolation boundary from other k8s objects
we can limit the resources like CPU,Momeory on per namespace basis (resource quota)



Step-01: Introduction
Namespaces allow to split-up resources into different groups.
Resource names should be unique in a namespace
We can use namespaces to create multiple environments like dev, staging and production etc
Kubernetes will always list the resources from default namespace unless we provide exclusively from which namespace we need information from.

Step-02: Namespaces Generic - Deploy in Dev1 and Dev2
Create Namespace
# List Namespaces
kubectl get ns 

# Craete Namespace
kubectl create namespace <namespace-name>
kubectl create namespace dev1
kubectl create namespace dev2

# List Namespaces
kubectl get ns 
Comment NodePort in UserMgmt NodePort Service
File: 07-UserManagement-Service.yml
Why?:
Whenever we create with same manifests multiple environments like dev1, dev2 with namespaces, we cannot have same worker node port for multiple services.
We will have port conflict.
Its good for k8s system to provide dynamic nodeport for us in such situations.
      #nodePort: 31231
Error if not commented
The Service "usermgmt-restapp-service" is invalid: spec.ports[0].nodePort: Invalid value: 31231: provided port is already allocated
Deploy All k8s Objects
# Deploy All k8s Objects
kubectl apply -f kube-manifests/ -n dev1
kubectl apply -f kube-manifests/ -n dev2

# List all objects from dev1 & dev2 Namespaces
kubectl get all -n dev1
kubectl get all -n dev2

***** Kubernetes Namespaces - LimitRange - Declarative using YAML
Step-01: Create Namespace manifest
Important Note: File name starts with 00- so that when creating k8s objects namespace will get created first so it don't throw an error.
apiVersion: v1
kind: Namespace
metadata:
  name: dev3

Step-02: Create LimitRange manifest
Instead of specifying resources like cpu and memory in every container spec of a pod defintion, we can provide the default CPU & Memory for all containers in a namespace using LimitRange
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ns-resource-quota
  namespace: dev3
spec:
  limits:
    - default:
        memory: "512Mi" # If not specified the Container's memory limit is set to 512Mi, which is the default memory limit for the namespace.
        cpu: "500m"  # If not specified default limit is 1 vCPU per container 
      defaultRequest:
        memory: "256Mi" # If not specified default it will take from whatever specified in limits.default.memory
        cpu: "300m" # If not specified default it will take from whatever specified in limits.default.cpu
      type: Container                        


Step-03: Update all k8s manifest with namespace
Update all files from 02 to 08 with namespace: dev3 in top metadata section in folder kube-manifests/02-Declarative
Example
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-mysql-pv-claim
  namespace: dev3

Step-04: Create k8s objects & Test
# Create All Objects
kubectl apply -f kube-manifests/

# List Pods
kubectl get pods -n dev3 -w

# View Pod Specification (CPU & Memory)
kubectl get pod <pod-name> -o yaml -n dev3

# Get & Describe Limits
kubectl get limits -n dev3
kubectl describe limits default-cpu-mem-limit-range -n dev3

# Get NodePort
kubectl get svc -n dev3

# Get Public IP of a Worker Node
kubectl get nodes -o wide

# Access Application Health Status Page
http://<WorkerNode-Public-IP>:<NodePort>/usermgmt/health-status

Step-05: Clean-Up
Delete all k8s objects created as part of this section
# Delete All
kubectl delete -f kube-manifests/
References:
https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/


***** Kubernetes Namespaces - LimitRange - Declarative using YAML
Step-01: Create Namespace manifest
Important Note: File name starts with 00- so that when creating k8s objects namespace will get created first so it don't throw an error.
apiVersion: v1
kind: Namespace
metadata:
  name: dev3


Step-02: Create LimitRange manifest
Instead of specifying resources like cpu and memory in every container spec of a pod defintion, we can provide the default CPU & Memory for all containers in a namespace using LimitRange
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ns-resource-quota
  namespace: dev3
spec:
  limits:
    - default:
        memory: "512Mi" # If not specified the Container's memory limit is set to 512Mi, which is the default memory limit for the namespace.
        cpu: "500m"  # If not specified default limit is 1 vCPU per container 
      defaultRequest:
        memory: "256Mi" # If not specified default it will take from whatever specified in limits.default.memory
        cpu: "300m" # If not specified default it will take from whatever specified in limits.default.cpu
      type: Container                        


Step-03: Update all k8s manifest with namespace
Update all files from 02 to 08 with namespace: dev3 in top metadata section in folder kube-manifests/02-Declarative
Example
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-mysql-pv-claim
  namespace: dev3

Step-04: Create k8s objects & Test
# Create All Objects
kubectl apply -f kube-manifests/

# List Pods
kubectl get pods -n dev3 -w

# View Pod Specification (CPU & Memory)
kubectl get pod <pod-name> -o yaml -n dev3

# Get & Describe Limits
kubectl get limits -n dev3
kubectl describe limits default-cpu-mem-limit-range -n dev3

# Get NodePort
kubectl get svc -n dev3

# Get Public IP of a Worker Node
kubectl get nodes -o wide

# Access Application Health Status Page
http://<WorkerNode-Public-IP>:<NodePort>/usermgmt/health-status


Step-05: Clean-Up
Delete all k8s objects created as part of this section
# Delete All
kubectl delete -f kube-manifests/
References:
https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/


***** Resource quota

