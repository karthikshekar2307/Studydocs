Section 11 - Troubleshooting
-----------------------------

***** Application Failure

Check Accessibility
For example on a 2 tier application, DB and Web, Database POD hosts DB and serves web via web-service

First, check app frontend
If its web via
# curl http://web-service-ip:node-port

Next, check service if its covered endpoints for web podd
# kubectl describe service web-service

Compare the selectors on service with pod

Next check pod
# kubectl get pod
- review the number of restarts

Check events related to pod via describe
# kubectl describe pod web

Logs
# kubectl logs web

# kubectl logs web -f -> to watch

Next, check status of DB service


**** Practise test
q1.
# kubectl -n alpha get all
(name of mysql service is not correct)
# kubectl -n alpha get svc mysql -o yaml > mysql-service.yaml
# kubectl -n alpha delete svc mysql
edit yaml file and set name to mysql-service
# kubectl create -f mysql-service.yaml

q2.
Check beta namespace
# kubectl -n beta get all
# kubectl -n beta get svc
# kubectl -n beta describe mysql-service -n beta
# kubectl -n beta get svc mysql-service -o yaml > mysql-service.yaml
# kubectl -n beta delete svc mysql-service
# kubectl -n beta create -f mysql-service
Change targetport - 3306


q3.
# kubectl -n gamma get all
See endpoints
# kubectl -n gamma get ep
# kubectl get svc mysql-service -n gamma -o yaml
( selector is incorrect)
# kubectl delete svc mysql-service -n gamma
# kubectl -n gamma expose mysql --name=mysql-service
# kubectl -n gamma get ep


q4.
(need to update to root from sqluser)
# kubectl -n delta describe deployment mysql

# kubectl -n delta get deployments webapp-mysql -o yaml > web.yaml
# kubectl delete deployment webapp-mysql -n delta
# vi web.yaml
(update db user name)
# kubectl create -f web.yaml


q5.
# kubectl -n epsilon get all
# kubectl -n epsilon describe deployment webapp-mysql
# kubectl -n epsilon get deployments webapp-mysql -o yaml > webapp.yam
(update mysql user)
# kubectl -n epsilon delete deployment webapp-mysql
# kubectl create -f webapp.yaml
# kubectl describe pod webapp-mysql -n epsilon
(update password)


q6.
(unable to reach nodeport/application port)
# kubectl -n zeta get svc web-service
(node port is wrong)
# kubectl get svc web-service -n zeta -o yaml > web-service.yaml
# vi web-service.yaml
(update node port)
# kubectl create -f web-service.yaml
# kuebctl -n zeta get deployments webapp-mysql > web.yaml
# kubectl -n zeta get pod mysql -o yaml > web.yaml
# vi web.yaml
(update db user to root)
(update correct password)
# kubectl -n zeta delete deployments webapp-mysql
# kubectl create -f mysql.yaml


***** Control Plane failure

- Check node status
# kubectl get nodes

- Check pods
# kubectl get pods

- Check all control plane are running
# kubectl get pods -n kube-system


- Check logs
# kubectl logs kube-apiserver-master
# journalctl -u kube-apiserver


***** Practise test - Control plane failure
check existing deployment
# kubectl get all

# kubectl describe pod <pod>

# kubectl get pods -n kube-system

# kubectl -n kube-system describe pod kube-scheduler

# cd /etc/systemd/system/kubelet.service.d
# ls
# more /var/lib/kubelet/config.yaml
# vi kube-scheduler.yaml
(correct the typo)


b. Scale the deployment
# kubectl scale deployment app --replicas=2

c. even after scale, replica did not increase
# kubectl get pods -n kube-system
# kubectl -n kube-system describe kube-controller-manager.yaml
# kubectl -n kube-system logs kube-controller-manager
# cd /etc/kubernetes
# vi kube-scheduler-managet
<change file name>

d. scaling did not work again
unable to get client CA file
# cd /etc/kubnernetes/manifests
# vi kube-controller-manager
correct the hostpath


***** Worker Node failure
- check node status
- kubectl describe node worker-1
- top
- service kubelet status

***** Practise test
documentation: troubelshooting clusters
a. Fix broken cluster
# kubectl get nodes
# ssh node01
# ps -ef |grep -i kubelet
# systemctl status kubelet
# systemctl restart kubelet
# kubectl get nodes

b. Cluster broken again
# kubectl describe node node01
# kubectl status kubelet.service
# ssh node01
# systemctl status kublet.service -l
# journalctl -u kubelet
< scroll down to bottom>
# cd /etc/systemd/system/kubelet.service.d
# cat 10-kubeadm.conf
< check kubelet config file>
# vi /var/lib/kubelet/config.yaml
# cd /etc/kubernetes/pki
# systemctl daemon-reload
# systemctl restart kubelet


c. cluster is broken again
# kubectl get nodes
# kubectl describe node node01
# ssh node01
# systemctl status kubelet
# kubectl cluster info (kube master (api server) will run on 6443)
# cd /etc/systemd/system/kubelet.service.d
check for kubeconfig
correct the port
# systemctl daemon-reload
# systemctl restart kubectl
# kubectl get nodes


**** Network Troubleshooting
Network Plugin in kubernetes
--------------------

Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration.

- cni-bin-dir:   Kubelet probes this directory for plugins on startup

- network-plugin: The network plugin to use from cni-bin-dir. It must match the name reported by a plugin probed from the plugin directory.



There are several plugins available and these are some.



1. Weave Net:



  These is the only plugin mentioned in the kubernetes documentation. To install,

kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"



You can find this in following documentation :

                  https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/



2. Flannel :



   To install,

  kubectl apply -f               https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

   

Note: As of now flannel does not support kubernetes network policies.



3. Calico :

   

   To install,

   curl https://docs.projectcalico.org/manifests/calico.yaml -O

  Apply the manifest using the following command.

      kubectl apply -f calico.yaml

   Calico is said to have most advanced cni network plugin.



In CKA and CKAD exam, you won't be asked to install the cni plugin. But if asked you will be provided with the exact url to install it. If not, you can install weave net from the documentation 

      https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/



Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.





DNS in Kubernetes
-----------------
Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.



Memory and Pods

In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance.



Kubernetes resources for coreDNS are:   

a service account named coredns,

cluster-roles named coredns and kube-dns

clusterrolebindings named coredns and kube-dns, 

a deployment named coredns,

a configmap named coredns and a

service named kube-dns.



While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.



Port 53 is used for for DNS resolution.



    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }


This is the backend to k8s for cluster.local and reverse domains.



proxy . /etc/resolv.conf



Forward out of cluster domains directly to right authoritative DNS server.





Troubleshooting issues related to coreDNS
1. If you find CoreDNS pods in pending state first check network plugin is installed.

2. coredns pods have CrashLoopBackOff or Error state

If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options:

a)Upgrade to a newer version of Docker.

b)Disable SELinux.

c)Modify the coredns deployment to set allowPrivilegeEscalation to true:



kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.



  There are many ways to work around this issue, some are listed here:



Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.

Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.

A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS, for example forward . 8.8.8.8. But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.



3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.

              kubectl -n kube-system get ep kube-dns

If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.





Kube-Proxy
---------
kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.



In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.



kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.



If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container.



    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
 

    So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

 

  In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

 

Troubleshooting issues related to kube-proxy
1. Check kube-proxy pod in the kube-system namespace is running.

2. Check kube-proxy logs.

3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.

4. kube-config is defined in the config map.

5. check kube-proxy is running inside the container

# netstat -plan | grep kube-proxy
tcp        0      0 0.0.0.0:30081           0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 172.17.0.12:33706       172.17.0.12:6443        ESTABLISHED 1/kube-proxy
tcp6       0      0 :::10256                :::*                    LISTEN      1/kube-proxy




References:

Debug Service issues:

                     https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/

DNS Troubleshooting:

                     https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/



